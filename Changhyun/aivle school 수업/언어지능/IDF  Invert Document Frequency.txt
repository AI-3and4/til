IDF : Invert Document Frequency
여러 문서를 보았을 때 몇개의 문서에 걸쳐 나왔는가 => 많이 나올수록 대표성이 떨어짐
TF : 현재 문서에 있는단어의 대표성

TF-IDF의 경우 검색 엔진에서 많이 씀.
문서 하나하나마다 신경망이나 생성망을 박아놓을 수는 없이 때문에 TF-IDF를 이용해서 문서를 인덱싱 해놓음.
이후 검색하면 빠르게 꺼내줄 수 있음.
50년이 넘었는데도 아직도 쓸 정도로 powerful한 알고리즘

요즘 인터넷 기사는 RSS를 제공함. 기사에 대해서 XML을 같이 제공해주는 거임.
저거 이용해서 검색할거임
예로 IT 뉴스 중 검색할 뉴스를 뽑고 싶다 하면 거기 있는 IT뉴스 가져오면 됨.


단어의 표현은 벡터로 이루어짐. 
=>하나의 벡터를 해당 단어를 표현하는 일종의 유니크한 feature (다른 수강생의 표현)
이유 : 
단어가 만약 단일 스칼라로 표현 될 시, 표현에 한계가 너무 큼.

벡터로 표현하는 방법
원핫인코딩은 아님. 원핫인코딩할 시 단어의 특성을 표현할 수 없음.
더군다나 단어의 수가 매우 많으므로, 상당히 비효율적임


벡터로 표현 이후 이를 검색할 때에는 단어의 유사성을 보기 위해 벡터 유사도를 사용
(Jaccard index or Accuracy score, 코사인 유사도, 유클리디언 유사도, 맨하탄 유사도)

코사인 유사도는 문장이 유사하면 1, 유사하지 않으면 0에 가까워짐. feature vector matrix는 음수값이 없기 때문에 유사도가 음수인 경우는 존재하지 않음.


KNN과 K-Mean Clustering 등의 분류를 위한 비지도 알고리즘 했음.
이때 졸았으므로 한번 더보기

Word2Vec : 각 단어를 벡터 형태로 임베딩.
이때 만약 원핫 인코딩을 핳 경우, 공간상의 문제 뿐 아니라, 임베딩된 각 벡터가 직교하기 때문에 유사도를 확인할 수 없음.




