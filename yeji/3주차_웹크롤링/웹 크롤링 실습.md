# 웹크롤링 실습

## 동적 페이지 / 정적 페이지 구분

직접 트래픽을 보고 확인하기
-> 클릭 이벤트 등의 이벤트가 있는지의 여부로 판단

- URL 및 트래픽 확인 방법
    1. F12 누르고 Network 탭 클릭
    
    ![image.png](images/image%201.png)
    
    1. '1' 누르면 request 요청 내역 초기화
    
    이때, 클릭이벤트 발생하며 새로운 request('2')가 생긴다면 동적페이지!
    
    ![image.png](images/image%202.png)
    
    클릭 이벤트 및 request 없이 바로 데이터가 출력된다면 정적페이지!
    
    ![image.png](images/image%203.png)
    

## 동적 페이지에서의 웹 크롤링 (request, json)

```python
# 1. URL 가져오기
url = f'https://m.stock.naver.com/api/index/{code}/price?pageSize={page_size}&page={page}'
    
# 2. URL 요청 후 응답 받기
response = requests.get(url)
    
# 3. 응답 데이터 파싱
data = response.json() # 결과를 json 타입으로 변환
pd.DataFrame(data)[['localTradedAt', 'closePrice']] # 데이터 프레임으로 변경 및 원하는 데이터 추출
```

결과

![image.png](images/image%204.png)

## 정적 페이지에서의 웹 크롤링 (bs4, css-selector)

```python
# 1. URL 가져오기
url = f"https://search.naver.com/search.naver?query={query}"

# 2. URL 요청 후 응답 받기
response = requests.get(url)
    
# 3. 응답 데이터 파싱
# 3-1. Beautiful Soup 객체 생성
dom = BeautifulSoup(response.text, "html.parser")

# 3-2. css-selector로 원하는 엘리먼트 선택
elements = dom.select(".fds-refine-query-grid a")

# 3-3. 각 엘리먼트에서 text 데이터 수집
keywords = []
for element in elements:
    keyword = element.text
    keywords.append(keyword)
print(keywords)

# 3-4. DataFrame으로 바꾸기
df = pd.DataFrame({"keywords": keywords})
```

결과

![image.png](images/image%205.png)

### css-selector : selector 가져오기
    
![image.png](images/image%206.png)
    
주의) 크롬의 css-selector로 가져올 경우 불필요한 태그를 많이 포함함
```    
ex. #fdr-8434e5a1f77a44639b9a57c6336c23d9 > div > div > div.bAUlEC095ho5bG7U0km1.fds-grid-layout-keyword.fds-refine-query-grid
```

실제로는 `.fds-refine-query-grid` 만 사용해도 됨!
    

## WAS에서 데이터 수집 차단했을 경우 해결 방법

문제상황 : response의 status code가 403 또는 500인 경우

→ 웹 서버에서 데이터 수집이 안되도록 설정된 것

해결 : header 정보 설정한 후 request에 header 포함해서 요청

```python
url = "https://finance.daum.net/api/exchanges/summaries"

# header 설정 : User-Agent 설정하기
headers = {
'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',
'Referer': 'https://finance.daum.net',
}
# response = requests.get(url)
response = requests.get(url, headers=headers)
```

- User-Agent : 웹서비스에 접속하는 Client의 환경정보
    - 브라우저 및 OS에 대한 정보
    - `ex. Windows, chrome browser` → 정상적인 사용자라고 판단하여 `status code 200`으로 응답
    - `ex. python` → 정상적이지 않은 사용자라고 판단하여 `status code 403`
    - 해당 값은 client 쪽에서 만드는 것이므로 값 임의로 설정 가능!!
        - 크롬 브라우저 값 넣기
            
            dev tool에서 `request → Headers → User-Agent` 값 참고하기
            
            ![image.png](images/image%207.png)
            
- Referer : 웹서비스에 접속하기 이전의 웹서비스에 대한 URL

### 추가할 내용

1. rest api 사용하는 경우
    
    ex. 직방
    
2. html 구성요소 및 구조